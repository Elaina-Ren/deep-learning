老师同学们大家好，我们小组在resnet18的baseline上实现了gcnet、res2net、tripletattention、coordinateattention四种方法并作出了改进
首先我来介绍一下baseline，我们本身由于电脑性能限制，只能采用深度较浅的使用basic block的resnet18这一网络，但由于我们在复现res2net时发现它使用了在resnet50上采用的bottleneck网络基本块。二者的差别如图，basicblock使用2个3*3的卷积层，而bottleneck是1*1+3*3+1*1。为了使方便，我们使用了8个bottleneck作为基本块。
我们首先复现了GCnet，gc是Global Context 全局上下文的简称，他的出发点是长距离依赖，即在一个序列或文本中，两个远距离位置之间存在着依赖关系或相互影响的情况。网络吸收了对与query独立的全局上下文进行建模的SENet和采用自注意力机制来对query成对关系进行建模的NLNet，进行简化与统一。其贡献有1、上下文建模模块：将所有位置的特征聚合形成全局上下文特征2、特征转换模块：对通道级别的相关性进行捕捉3、融合模块：将全局上下文特征合并到所有位置的特征。其发展背景是1、nlblock2、简化nlblock3、seblock...见ppt 对于网络结构，可以看出gcnet包含一个简化NLblock的1x1conv和softmax层，也包含seblock除池化外的模块。参考资料不赘述
我们还复现了res2net，他是一个多尺度的主干网络。它的特点包括在单个残差块内构造具有等级制的类似残差连接、在粒度级别表示多尺度特征，并增加了每个网络层的感受野。虽然是resnet50，但我们在resnet18上运行。如图所示，该网络将bottleneck的中间的3x3层换成了方框中部分，让部分数据经过0层-3层，扩大了感受野。
然后，我们复现了triplet attention，这一网络相比传统网络和cbam通过捕捉空间维度和输入张量通道维度之间的交互作用，来建模通道注意和空间注意，增强二者交互性。它分为3个分支，分别计算了通道注意力、通道C和空间W（宽度）维度交互捕获、通道C和空间H（高度）维度交互捕获分支最后对3个分支输出特征进行相加求Avg。我们将其插进了bottleneck的第三个卷积层中。
最后，我们复现了coordinate attention。这是一种对注意力机制的改进，其具体操作如图所示，其思想是分别计算宽度和高度，生成宽度和高度方向上带有注意力权重的特征图。复现：在bottleneck结构中第二个卷积层conv2进行特征提取之后插入了一个CA模块
我们在cifar100数据集下，以这样一组参数下得到了这样的实验结果。可以看出，四种方法均有提升，triplet最多，ca最少
除此知网，我们进行了以下创新。
首先是CA+GC的组合，将全局上下文信息与空间信息更好地结合， 结果不乐观，如图，反而不如gcnet。
然后是ca+channelattention，我们认为可以进一步增强CA模块的对通道信息的建模能力。设计左图轻量级的通道注意力建模模块，在ca后加入channel层，获得了提升。
然后是gcnet+，这一方法为不同bottleneck提出了不同权重。为此考虑使用全连接层对高级和低级特征的global context之间的非线性关系进行approximate。实验发现改进后的模型在Cifar100分类任务上取得更好的性能，缺点是模型复杂度提升，训练速度减慢；且任务是在相对简单的数据集上进行训练与评估，未来需要思考如何进一步考虑特征和全局上下文的内在关系，并降低模型复杂度，以取得更好的泛化效果。
总的来说，我们本次实验我们在baseline的基础上共复现了4篇论文，并在做出较为充分的消融实验工作后提出2种改进网络的创新方法，对于以后工作，我们打算采用更复杂的网络以提高网络表现和探索更具创新性的网络架构实现。
谢谢大家。








